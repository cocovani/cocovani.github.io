## MusicVAE
## [논문] A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music. (Roberts et al.)

본 논문에서 소개하는 **MusicVAE**는, 악보 데이터 를 이용하여 음악을 보다 잘 모델링하기 위한 고민으로부터 고안되었다. 이 모델은 생성 모델(Generative model)의 일종으로, 생성 모델은, 예를 들어, 변수 x의 확률 분포 $p(x)$를 이용하여 데이터 x를 생성해낼 수 있는 모델을 일컬으며, 종종 판별 모델(Discriminative)과 비교되어 설명되곤 한다. 

<p align="center"><img src="/assets/images/9.png" width="40%" height="40%"/> </p>

판별 모델이 주어진 데이터가 어떤 클래스에 속하는지 ‘판별’하는 목적을 지닌다면, 생성 모델은 주어진 데이터의 분포를 학습하고, 그 분포를 기반으로 새로운 데이터를 만들어낼 수 있도록 하는 것을 목적으로 한다.

MusicVAE는 최근 딥러닝과 결합되어 높은 성능을 보이고 있는 여러 생성 모델 중에서, 이름에서 알 수 있듯이, **VAE**(Variational Autoencoder)를 응용한다. 

**VAE (Variational Autoencoder)**

<p align="center"><img src="/assets/images/3.png" width="40%" height="40%"/> </p>

입력값을 처리하여 다시 똑같이 만들어 출력하는 것을 목적으로 하는 **AE(Autoencoder)**를 근간으로 하는 VAE는, 변수  $x$ 의 확률 분포인 $p(x)$ 뿐만 아니라, 잠재 변수(latent variable) $z$에 대한 $p(z)$, $p(z|x)$ 까지 모델링할 수 있다는 점에서 차별성과 강점을 갖는다. 잠재 변수 $z$ 는 기존 데이터 또는 잠재 공간(latent space) 분포로부터 얻어지는 변수인데, 이것이 주어진 데이터의 변별력 정보를 가질 수 있고, 이는 데이터의 속성 정보로 활용될 수도 있기 때문이다. 이 속성 정보들은 새로운 데이터를 생성해내는 데에도 이용될 수 있다.

위 그림에서 볼 수 있듯, VAE 인코더는 평균 벡터(μ)와 표준편차 벡터(σ) 세트를 만들어 잠재 공간의 분포 정보를 전달한다.


<div align="center">
💡 VAE의 데이터 생성 모델: z ∼ p(z), x ∼ p(x|z)

  VAE 구성 (c.f. 위 그림과 표기 다름) :
<br>인코더 $q_λ$($z|x$) : posterior $p(z|x)$ 계산
<br>디코더 $p_θ$($x|z$) : likelihood $p(x|z)$ 파라미터화
<br>λ , θ : 뉴럴 네트워크상 인코더와 디코더 각각의 파라미터 
</div>


다만, 기존의 VAE(Variational Autoencoder)들은 짧은 시퀀스 데이터를 처리하는 데에 있어서는 좋은 성능을 보이지만, 호흡이 긴 시퀀스 데이터들을 다룰 때엔 학습 과정에서 잠재 변수(latent variable)가 무시되는 “사후 붕괴(posterior collapse)” 문제에 직면하게 된다.

예를 들어, 회귀 인코더(recurrent encoder)와 디코더를 사용하는 회귀 VAE (recurrent VAE)의 디코더(simple stacked RNN)는 그 자체로 시퀀스 처리에 강력하기 때문에, 디코더의 초기 상태(initial state)로 쓰이기 위해 전달되는 잠재 코드를 무시할 수 있다 (vanishing influence of the latent state). 뿐만 아니라, 본 연구에서는 모델이 시퀀스를 단일 잠재 벡터로 축소시키는 일이 필요한데, 앞의 모델은 모델링 과정에서 오히려 시퀀스 길이가 길어진다는 단점을 갖는다.

본 연구에서 주재료로 다루려는 음악 데이터, **악보**는 긴 시퀀스 데이터로 이루어지기 때문에, 기존 모델들로 학습하는 데에는 한계가 있다. 따라서, 기존 VAE보다 효과적인 장기 시퀀스 모델링을 위하여, 본 연구는 계층(hierarchical RNN) 디코더를 사용하는 VAE 모델을 제안한다. 계층 디코더는 전체 입력 시퀀스를 서브시퀀스(subsequences)들로 분할하고, 이 서브시퀀스들에 대한 임베딩을 이용해 다시 서브시퀀스들을 생성한다.

**데이터로서의 음악**

올바른 모델링에 앞서, 음악 데이터가 다른 데이터들과 달리 갖는 특성을 먼저 짚어볼 필요가 있다. 

첫째, 음악은 대체로 긴 시퀀스들로 이루어지는 데이터인데, 그 자체로도 **계층적(hierarchical)**이라 할 수 있다. 하나의 곡은 섹션으로 분할될 수 있고, 섹션은 다시 마디로 분할될 수 있으며, 마디 안에는 박자들이 포함되는 식이다.

둘째, 음악은 단일 스트림이 아니다. 하나의 곡에는 대체로 여러 개의 스트림(플레이어, 악기 등)이 존재하고, 각 스트림 사이에는 의존성이 존재한다. 음악은 멀티스트림 데이터이다.

---

### MusicVAE

<p align="center"><img src="/assets/images/1.png" width="40%" height="40%"/> </p>


MusicVAE의 인코더로서는, 입력값에 대한 보다 긴 컨텍스트 정보를 잠재 분포 파라미터화시 제공할 수 있도록 2-레이어 Bidirectional LSTM 레이어가 사용된다. 입력 시퀀스 $x = {x1,x2,...,xT} t$ 를 이용해 얻어지는 최종 상태 벡터를 연쇄시켜, fully-connected 레이어에서  잠재 분포 파라미터 μ 와 σ 를 얻을 수 있다.

<p align="center"><img src="/assets/images/7.png" width="40%" height="40%"/> </p>


앞서 강조했듯, 디코더는 계층적 레이어를 사용한다. 입력 시퀀스 $x$  는 $U$ 개의 겹침 없는 subsequences  $Y_u$ 로 분할되고, endpoint는 $i_u$ 이다.

<p align="center"><img src="/assets/images/10.png" width="40%" height="40%"/> </p>


잠재 벡터 $z$ 는 $tanh$  활성함수를 거쳐 RNN 컨덕터(c.f. unidirectional LSTM)의 초기 상태(initial state)로 전달되어, $U$ 개의 임베딩 벡터 $c = {c_1, c_2, . . . , c_U }$ 를 생산한다 ($c$ 는 각각의 서브시퀀스에 대응됨). 임베딩 벡터 $c$ 들이 RNN 디코더(2-레이어 LSTM)로 전달되어, $Y_u$ 서브시퀀트 출력 토큰들에 대한 분포 시퀀스를 생산하는 데에 이용된다. 

** 잠재 상태(latent state)를 반드시 반영시키기 위해서, 본 모델은 각 RNN 레이어가 출력 서브시퀀스 내에서만 상태를 전파할 수 있도록 유효 범위를 제한한다. 따라서, 오로지 잠재 코드에 의존하는 컨덕터 임베딩을 통해서만 디코더가 컨텍스트 정보를 얻을 수 있게 된다. 

---

### **실험**

데이터셋: MIDI 파일 (2마디/16마디 멜로디, 2마디/16마디 드럼 패턴, 16마디 “trio(멜로디, 베이스 드럼)” 시퀀스)

<aside>
💡 멜로디, 베이스라인 : 16분음표 시퀀스로 처리됨. 128개 note-on 토큰, note-off 토큰, 쉼표(rest) 토큰.
드럼 패턴 : 61개 드럼 클래스가 9개 클래스로 매핑됨 (c.f. General MIDI standard). 512개 카테고리 토큰.
타이밍 : 각 마디는 16개 이벤트를 포함 (즉, 2마디 데이터의 T=32, 16마디 데이터의 T=256)
U = 16 :한 마디가 하나의 서브시퀀스에 대응

</aside>

한편, 악보의 다중 스트림 특성을 반영하려는 고민에 따라, “trio” 데이터셋도 함께 실험되었다. 동일한 MusicVAE가 이용되었고, “trio”는 드럼, 베이스, 멜로디 3개 악기에 대한 토큰 분포를 출력한다. 3개 악기의 악보가 수직으로 분할된 서브시퀀스들이 각각 디코더로 전달된다. 

**실험 결과**

1. recurrent VAE vs. hierarchical MusicVAE
    
<p align="center"><img src="/assets/images/4.png" width="40%" height="40%"/> </p>    

기존 회귀 VAE (recurrent VAE)는 짧은 시퀀스에 대해서는 높은 재건(reconstruction) 성능을 보였지만, 긴 시퀀스(16 마디)에 대해서는 전부 계층 VAE(Hierarchical VAE) 보다 저조한 성능을 보였다. 두 모델 비교 외에도, 기본 샘플링과 티처-포싱(teacher-forcing)의 두 가지 방식에 따른 성능 역시 비교되었다. 위 테이블에서 볼 수 있듯, 기본 샘플링과 티처-포싱 사이에 큰 차이가 보이지 않는 결과가 나타났고 (≈5 − 11%), 이는 계층 VAE의 모델링 결과의 우수함을 의미한다. 

1. “trio” 데이터셋 (멜로디, 베이스, 드럼의 16마디 시퀀스) 학습

싱글 스트림 데이터에 대한 학습 결과와 마찬가지로, “trio” 데이터셋에 대해서도 MusicVae가 베이스라인보다 더 높은 성능을 보였다. 기본 샘플링과 티처-포싱 사이의 차이도 크지 않았다.

1. 보간 (Interpolations)

보간이란, 주어진 데이터들에 대한 값들로부터 함수를 구하고, 이를 이용해 데이터 주변의 미지의 값들을 추정해가는 것이다. 본 연구와 관련해서는, 음가 데이터들 사이의 빈 지점들을 부드럽게 채워나가는 방식이라고 해석해 볼 수 있다. (여기에서는 특히, 데이터 지점 구간들 사이에 대한 보간 (c.f. extrapolation 과 비교))

<p align="center"><img src="/assets/images/5.png" width="40%" height="40%"/> </p>


위 그래프는 기존 디코더와 계층 디코더 및 베이스라인으로부터 도출된 잠재 공간 보간(interpolation) 표현으로, 평가(evaluation) 데이터셋 내 1024개의 16마디 멜로디(A)와, 다른 1024개의 멜로디(B) 사이의 보간 상태에 대한 평균이다. 베이스라인은 두 시퀀스를 단순히 결합하여 베르누이 랜덤 변수를 샘플링한 “Data” 보간이다. 

X축은, 좌에서 우로, A와 B 시퀀스 사이의 보간을 의미한다.

위쪽의 그래프는 시퀀스A와 보간 사이의 해밍 거리(Hamming distance)가 모든 방법에서 증가하며, 복호화된 시퀀스들이 시퀀스A보다 시퀀스B에 부드럽게 가까워지는 것을 보여준다. 이를 통해, 재건되는 데이터들이 계속 유지되지 않고, 다른 유형으로 바뀐다는 것을 알 수 있다.

아래쪽 그래프를 위해서는, 데이터셋에 언어 모델을 적용하는 작업이 이루어졌다. 5-그램 모델은, 연속되는 5개의 데이터 토큰들을 단위로 학습된 모델을 의미한다. 그래프는 보간된 시퀀스들 각각에 대한 (정규화된) 코스트를 보여주고, 이 값들은 다음으로부터 계산되었다.

<p align="center"><img src="/assets/images/8.png" width="40%" height="40%"/> </p>


여기에서 $C_α$ 는 보간량 α 를 갖는 보간 시퀀스에 대한 언어 모델의 코스트를, $C_A$ 와 $C_B$ 는 엔드포인트 시퀀스 A와 B에 대한 코스트를 가리킨다. 언어 모델에 따르면, 보간된 시퀀스들은 본래 멜로디에 비해 매우 특이하게 나타난다.

1. 속성 벡터 연산

속성 벡터를 이용해 주어진 시퀀스의 속성을 바꿀 수 있도록 하기 위하여, 잠재 공간의 구조를 이용할 수 있는데, 이에 MIDI 악보로부터 얻을 수 있는 5개 속성이 이용되었다: C 다이아토닉 노트, 음 밀도 (note density), 평균 인터벌, 16분음표와 8분음표 당김음(syncopation). 

 이 속성들을 랜던 샘플들 속에서 측정하여 속성 벡터로 변환한 뒤, 각 속성에 대해 벡터를 가감하며 평균 백분율 변화를 측정한 결과, 두 가지 흥미로운 점을 도출할 수 있었다: 첫째, 주어진 속성 벡터는 타겟 벡터를 지속적으로 의도대로 변화시킨다는 점, 둘째, 어떤 속성이 강해질 때, 다른 속성이 약해지는 경우가 있다는 점.

1. 청음 테스트

멜로디, “trio”, 드럼 패턴에 대해서 청음 테스트가 실시되었다. 참가자들은 데이터셋과 모델에서 각각 하나씩 가져온 2개의 16마디 음악 셋을 듣고, 둘 중 어느 쪽이 더 음악성을 갖는지 평가했다.

<p align="center"><img src="/assets/images/2.png" width="40%" height="40%"/> </p>


그 결과, 계층적 디코더를 사용한 모델이 만든 샘플들이 기존 모델이 만든 샘플들보다 월등히 높은 점수를 받았고, MusicVAE 샘플이 실제 음악보다 더 높은 음악성을 가진 것으로 평가 받는 경우도 발견되었다.

Kruskal-Wallis H 테스트를 통해, 상기 모델들 사이에 통계적으로 유의미한 차이가 있음을 진단할 수도 있었다.

멜로디 : $χ^2(2)$ = 37.85, p < 0.001

트리오 : $χ^2(2)$ = 76.62, p < 0.001

드럼 : $χ^2(2)$ = 44.54, p < 0.001

윌콕슨부호순위검정 (Wilcoxon signed-rank test) 결과에서는, 참가자들이 MusicVAE의 샘플들 및 실제 음악들이 기존 모델의 샘플들보다 더 음악성 있다고 느낀다는 것을 발견했다 (각각 p < 0.01/3). 이 검정 결과에 따르면, MusicVAE의 샘플들은 실제 데이터와 유사한 정도의 음악성을 갖는다.

### 결론

결론적으로, 음악 데이터와 같이 긴 시퀀스 데이터들을 학습하는 데에 MusicVAE가 기존의 VAE 모델들보다 더 유리하다고 보여진다.
